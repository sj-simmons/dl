{"componentChunkName":"component---node-modules-rocketseat-gatsby-theme-docs-core-src-templates-docs-query-js","path":"/setup/experimentation/","result":{"data":{"mdx":{"id":"6d645170-00cf-51c8-87af-9dd4852b48fa","excerpt":"Typically, we want to train, validate, and test our models with various\nchoices of training hyper-parameters in an effort to optimize performance; andâ€¦","fields":{"slug":"/setup/experimentation/"},"frontmatter":{"title":"Running experiments","description":null,"image":null,"disableTableOfContents":null},"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Running experiments\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Typically, we want to\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"train, validate, and test our models with various\\nchoices of training hyper-parameters in an effort to optimize performance; and,\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"depending on our problem, experiment with tweaked or wholly different model architectures.\")), mdx(\"p\", null, \"Any ML framework or library facilitates your fine-tuning your training\\nprocess.  DUlib is, in addition, designed to assist the search for\\na preferred architecture for your problem.\"), mdx(\"p\", null, \"In any case, once you have your code working, you no doubt want to\\nrun some experiments.  There are several ways to do this. The following\\nare example solutions that we use at the DL@DU Project.\"), mdx(\"p\", null, \"Suppose that you have a program named \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"myprogram.py\"), \" that accepts\\nvarious commandline arguments. Maybe you run your program like this:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-shell\"\n  }), \"python3 myprogram.py -bs 20 -epochs 10 -lr 0.05 -mo 0.95\\n\")), mdx(\"p\", null, \"The sections that follow, we look at recipes for running experiments\\non various hardware configurations.\"), mdx(\"h2\", {\n    \"id\": \"single-node-multiple-gpus-one-user\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h2\"\n  }, {\n    \"href\": \"#single-node-multiple-gpus-one-user\",\n    \"aria-label\": \"single node multiple gpus one user permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Single node, multiple GPUs, one user\"), mdx(\"p\", null, \"Many of the advanced DL@DU students have accounts on the Dual-GPU ArchLinux machine\\nin Simmons' office (and they can rdp into that machine over a vpn).\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Scenario 1\")), mdx(\"p\", null, \"Suppose that you have a program that runs on a single GPU.  Since there are\\n2 GPUs in the Arch machine, you want to run 2 instances of your program in\\nparallel, one on each GPU. To keep things simple, suppose you just want\\nto tweak a few the learning parameters.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Method 1\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-shell\"\n  }), \"parallel -j2 eval CUDA_VISIBLE_DEVICES='$(( {%} - 1 ))' python3 myprogram.py -bs 20 -epochs 10 -lr ::: 0.05 0.01 -mo ::: 0.95 0.98\\n\")), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"The command above is equivalent to running (simultaneously) the following commands:\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-shell\"\n  }), \"CUDA_VISIBLE_DEVICES=0 python3 myprogram.py -bs 20 -epochs 10 -lr 0.05 -mo 0.95\\nCUDA_VISIBLE_DEVICES=1 python3 myprogram.py -bs 20 -epochs 10 -lr 0.01 -mo 0.98\\n\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Method 2\"), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"The following is just a different, but equivalent, way to organize Method 1.\"), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Create a file named, say, \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"params\"), \" containing these two lines:\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-text\"\n  }), \"-bs 20 -epochs 10 -lr .05  -mo .95\\n-bs 20 -epochs 10 -lr .01  -mo .98\\n\")), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Then run\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-shell\"\n  }), \"parallel -j2 eval CUDA_VISIBLE_DEVICES='$(( {%} - 1 ))' python3 myprogram.py {} :::: params\\n\")))), mdx(\"p\", null, \"Modifying Method 2, for instance, you can run many experiments. Suppose that the following\\nlines are in a file in your current directory called  \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"params2\"), \":\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-text\"\n  }), \"-bs 20  -epochs 10  -lr .001  -mo .99\\n-bs 20  -epochs 10  -lr .001  -mo .99  -channels 1 16 32\\n-bs 20  -epochs 10  -lr .001  -mo .99  -channels 1 32 16\\n-bs 20  -epochs 10  -lr .001  -mo .99  -channels 1 16 32 -widths 1000 500\\n\")), mdx(\"p\", null, \"Then, issuing the command\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-shell\"\n  }), \"parallel -j2 eval CUDA_VISIBLE_DEVICES='$(( {%} - 1 ))' python3 myprogram.py {} :::: params2\\n\")), mdx(\"p\", null, \"results in running (possibly in a different order, depending on which ones complete first):\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-shell\"\n  }), \"CUDA_VISIBLE_DEVICES=0 python3 myprogram.py -bs 20 -epochs 10 -lr 0.001 -mo 0.99\\nCUDA_VISIBLE_DEVICES=1 python3 myprogram.py -bs 20 -epochs 10 -lr 0.001 -mo 0.99 -channels 1 16 32\\nCUDA_VISIBLE_DEVICES=0 python3 myprogram.py -bs 20 -epochs 10 -lr 0.001 -mo 0.99 -channels 1 32 16\\nCUDA_VISIBLE_DEVICES=1 python3 myprogram.py -bs 20 -epochs 10 -lr 0.001 -mo 0.99 -channels 1 16 32 -widths 1000 500\\n\")), mdx(\"p\", null, \"Assuming the order above, whenever the first line finishes running on gpu 0, then the\\nthird line starts running on gpu 0, and similarly for gpu 1.\"), mdx(\"p\", null, \"You can capture the output (for either method) in a file by redirecting:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-shell\"\n  }), \"parallel -j2 eval CUDA_VISIBLE_DEVICES='$(( {%} - 1 ))' python3 myprogram.py {} :::: params2 >> logfile\\n\")), mdx(\"h2\", {\n    \"id\": \"single-node-multiple-gpus-multiple-users\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h2\"\n  }, {\n    \"href\": \"#single-node-multiple-gpus-multiple-users\",\n    \"aria-label\": \"single node multiple gpus multiple users permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Single node, multiple GPUs, multiple users\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Method 3\"), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"  Alternatively to the methods above, you could put these lines in a file called \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"jobs\"), \":\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-text\"\n  }), \"CUDA_VISIBLE_DEVICES=0 python3 myprogram.py -bs 20 -epochs 10 -lr 0.001 -mo 0.99\\nCUDA_VISIBLE_DEVICES=1 python3 myprogram.py -bs 20 -epochs 10 -lr 0.001 -mo 0.99 -channels 1 16 32\\nCUDA_VISIBLE_DEVICES=0 python3 myprogram.py -bs 20 -epochs 10 -lr 0.001 -mo 0.99 -channels 1 32 16\\nCUDA_VISIBLE_DEVICES=1 python3 myprogram.py -bs 20 -epochs 10 -lr 0.001 -mo 0.99 -channels 1 16 32 -widths 1000 500\\n\")), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"  and then simply issue the command\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-shell\"\n  }), \"parallel -j2 {} :::: params2 >> logfile\\n\")), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"  or, just,\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-shell\"\n  }), \"parallel -j2 -a params2 >> logfile\\n\")), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"But, since we are assuming multiple users, we may have more than person issuing\\ncommands like these &;mdash and that will very quickly lead to collisions.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Method 4\"), mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"This following is specific to the way that Simmons' office machine is setup.\"), \"\\n(Though Simmons will post the requisite code and details for setting things up this way.)\"), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"This method solves the problem of collisions by using a job queue. In other\\nwords, whenever you want to run programs that are GPU (or even CPU)\\nintensive, you encapsulate those in a file and put them on the queue. Then the\\nprograms are dequeued (in a FIFO manner) and run as resources become available.\"), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Let us write down how to do this on the Arch machine in Simmons' office. (Elsewhere,\\nwe will provide the code for doing this on your own machine.)\"), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Notice two things about the content of the file above:\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Each line  executes a program whose output is appended to a file called \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"log\"), \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The name of the program is comletely qualified (and \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"log\"), \" isn't).  We'll fix that below.\")), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"You place the four jobs in the file on the queue on Simmons' machine by\\nsimply appending those lines to a certain file on the Arch machine:\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-shell\"\n  }), \"cat jobs >> /home/sharedData/jobqueue\\n\")), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Of course, others might already have their jobs on the queue. You can check what's on the\\nqueue:\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-shell\"\n  }), \"cat /home/sharedData/jobqueue\\n\")), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Now, the lines in the file \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"/home/sharedData/jobqueue\"), \" remain even after\\nthose programs have completed. Over time, that file grows unless we remove\\nlines. Here is how we can do that.\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-shell\"\n  }), \"echo _clear >> /home/shared\\n\")), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"When the line \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"_clear\"), \" gets evaluated, the jobs above it (which have all already\\ncompleted) are removed. (If you wish, you can add _clear to your jobs file. Then\\nwhen you files complete, they will be removed from the queue.)\"), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Next, let's deal with lines in the file \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"jobs\"), \" above being unmanageably long.\\nYou can make \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"myprogram.py\"), \" executable with:\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-shell\"\n  }), \"chmod 755 myprogram.py\\n\")), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Then add the following line at the very beginning of \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"myprogram.py\"), \":\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-text\"\n  }), \"#!/usr/bin/env python3\\n\")), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Now your \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"jobs\"), \" file can be:\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-text\"\n  }), \"CUDA_VISIBLE_DEVICES=0 myprogram.py -bs 20 -epochs 10 -lr 0.001 -mo 0.99 >> log\\nCUDA_VISIBLE_DEVICES=1 myprogram.py -bs 20 -epochs 10 -lr 0.001 -mo 0.99 -channels 1 16 32 >> log\\nCUDA_VISIBLE_DEVICES=0 myprogram.py -bs 20 -epochs 10 -lr 0.001 -mo 0.99 -channels 1 32 16 >> log\\nCUDA_VISIBLE_DEVICES=1 myprogram.py -bs 20 -epochs 10 -lr 0.001 -mo 0.99 -channels 1 16 32 -widths 1000 500 >> log\\n_clear\\n\")))), mdx(\"h2\", {\n    \"id\": \"resources\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h2\"\n  }, {\n    \"href\": \"#resources\",\n    \"aria-label\": \"resources permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Resources\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"GNU Parallel \", mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://www.usenix.org/system/files/login/articles/105438-Tange.pdf\"\n  }), \"article\"))));\n}\n;\nMDXContent.isMDXComponent = true;","headings":[{"depth":2,"value":"Single node, multiple GPUs, one user"},{"depth":2,"value":"Single node, multiple GPUs, multiple users"},{"depth":2,"value":"Resources"}]}},"pageContext":{"slug":"/setup/experimentation/","prev":{"label":"DUlib","link":"/setup/dulib"},"next":{"label":"Intro and access ","link":"/content/access"},"githubEditUrl":"https://github.com/rocketseat/gatsby-themes/tree/master/examples/gatsby-theme-docs/src/docs/setup/experimentation.mdx"}},"staticQueryHashes":["1954253342","2360221584","2501019404","2679457992"]}